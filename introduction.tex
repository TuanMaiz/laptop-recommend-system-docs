\section{Introduction}
Artificial neural networks (ANN) have demonstrated strong performance in numerous tasks, but they consume enormous amounts of power due to intensive computational and memory demands. Spiking neural networks (SNNs), inspired by the human brain and offering energy-efficient alternatives. However, popular training methods for SNNs, Spatial Temporal Back-propagation (STBP) and ANN-SNN Conversion, both suffer from the challenge of inference latency. To address this issue, Hao et al. \cite{hao2024faster} proposed a parallel spiking framework. In this work, we adapt the Parallel Conversion (PC) framework for text type data tasks. We also validate the performance of PC on the widely used BERT model.